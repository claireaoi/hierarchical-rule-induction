{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pickle\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join as joinpath\n",
    "\n",
    "# %matplotlib inline\n",
    "%matplotlib qt\n",
    "pred_path='./Data/gqa/bg_pred.pickle'\n",
    "with open(pred_path, 'rb') as inp:\n",
    "    bg_pred_ls = pickle.load(inp)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "bg_pred_ls\n",
    "pred2idx = dict((pred, idx) for idx, pred in enumerate(bg_pred_ls))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def plot_dist_mat(embs, emb_name):\n",
    "    cnt_emb, cnt_feat = embs.shape\n",
    "    dist = torch.zeros((cnt_emb, cnt_emb))\n",
    "    assert len(emb_name) == cnt_emb\n",
    "    \n",
    "    for i, x in enumerate(embs):\n",
    "        for j, y in enumerate(embs):\n",
    "            dist[i, j] = torch.linalg.vector_norm(x - y).data.item()\n",
    "    \n",
    "    # print(dist)\n",
    "    # dist = nn.functional.normalize(dist, p=2, dim=1)\n",
    "    # dist=nn.functional.softmax(dist, dim=1)\n",
    "    # dist -= dist.min(1, keepdim=True)[0]\n",
    "    # dist /= dist.max(1, keepdim=True)[0]\n",
    "    # dist = nn.functional.softmax(dist, dim=1)\n",
    "    # print(dist)\n",
    "    rank_score, rank_id = torch.sort(dist, descending=True)\n",
    "    order = torch.zeros((cnt_emb, cnt_emb), dtype=int)\n",
    "    for row, rank in enumerate(rank_id):\n",
    "        for ord, pos in enumerate(rank):\n",
    "            # print(row, order, pos)\n",
    "            order[row, pos.data.item()] = ord\n",
    "    # rank_score = torch.sort(dist, descending=True)[0]\n",
    "    # print(rank_id)\n",
    "    # print(rank_score)\n",
    "    # print(order)\n",
    "    fig = plt.figure()\n",
    "    ax=fig.add_subplot(111)\n",
    "    cax = ax.matshow(order)\n",
    "    fig.colorbar(cax)\n",
    "    \n",
    "    plt.xticks(range(cnt_emb), emb_name)\n",
    "    plt.yticks(range(cnt_emb), emb_name)\n",
    "    plt.show()\n",
    "#     plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def get_closest_pairs(embs, emb_name, cnt, show_dist=False, dist_mode='cosine', norm='none'):\n",
    "    cnt_emb, cnt_feat = embs.shape\n",
    "    dist = []\n",
    "    \n",
    "    dist_val = torch.zeros((cnt_emb, cnt_emb))\n",
    "    for i, x in enumerate(embs):\n",
    "        for j, y in enumerate(embs):\n",
    "            if dist_mode=='cosine':\n",
    "                cos = nn.CosineSimilarity(dim=0)\n",
    "                dist_val[i, j] = cos(x, y)\n",
    "            else:\n",
    "                dist_val[i, j] = torch.linalg.vector_norm(x - y).data.item()\n",
    "    \n",
    "    # print(dist)\n",
    "    if norm == 'l2':\n",
    "        dist_val = nn.functional.normalize(dist_val, p=2, dim=1)\n",
    "    # dist=nn.functional.softmax(dist, dim=1)\n",
    "    \n",
    "    for i, x in enumerate(embs):\n",
    "        for j, y in enumerate(embs):\n",
    "            if i <= j:\n",
    "                continue\n",
    "            dist.append((dist_val[i][j].data.item(), (emb_name[i], emb_name[j])))\n",
    "    dist.sort(key=lambda x: x[0], reverse=True)\n",
    "    # rank_id = torch.sort(dist.view(-1), descending=True)[1]\n",
    "    # pair_ls = []\n",
    "    # for pos in rank_id[cnt_emb: cnt_emb+cnt]:\n",
    "    #     row = int(pos / cnt_emb)\n",
    "    #     column = pos % cnt_emb\n",
    "    #     pair_ls.append((emb_name[row], emb_name[column]))\n",
    "\n",
    "    cnt = min(cnt, len(dist))\n",
    "    if show_dist:\n",
    "        print(dist[: cnt])\n",
    "    else:\n",
    "        print([x[1] for x in dist[:cnt]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "nlp_model = GPT2LMHeadModel.from_pretrained('gpt2')  # or any other checkpoint\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "temp_index = tokenizer.encode('car', add_prefix_space=True)\n",
    "temp_feat = nlp_model.transformer.wte.weight[temp_index,:].shape[1]\n",
    "\n",
    "wn_pred_ls = []  # list of predicates aviliable in wordnet\n",
    "wn_pred_id = []\n",
    "wn_embs = []\n",
    "skip_pred_ls = []\n",
    "for i, pred in enumerate(bg_pred_ls):\n",
    "    # TODO: embedding pooling\n",
    "    wn_idx = tokenizer.encode(pred, add_prefix_space=True)\n",
    "    if len(wn_idx) > 1:\n",
    "        skip_pred_ls.append(pred)\n",
    "        continue\n",
    "\n",
    "    wn_pred_ls.append(pred)\n",
    "    wn_pred_id.append(i)\n",
    "    \n",
    "    t_emb = nlp_model.transformer.wte.weight[wn_idx,:]\n",
    "    wn_embs.append(t_emb)\n",
    "wn_embs = torch.cat(wn_embs, dim=0)\n",
    "# temp_predicates = PCA(n_components = self.num_feat-2*int(args.add_p0)).fit_transform(temp_predicates.detach().numpy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(len(wn_embs))\n",
    "print(wn_embs[0].shape)\n",
    "print(skip_pred_ls)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "191\n",
      "torch.Size([768])\n",
      "['cell_phone', 'covered_by', 'donut', 'faucet', 'giraffe', 'hanging_on', 'in_front_of', 'kite', 'license_plate', 'lying_on', 'mane', 'napkin', 'sitting_on', 'skateboard', 'skis', 'standing_in', 'standing_on', 'street_light', 'surfboard', 't-shirt', 'vase', 'walking_on', 'zebra']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# print(wn_embs.shape)\n",
    "tot=191\n",
    "plot_dist_mat(wn_embs[:tot,:].detach(), bg_pred_ls[:tot])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def load_model(model_dir, pred_name, use_gpu):\n",
    "    path = joinpath(model_dir, pred_name)\n",
    "    if use_gpu:\n",
    "        model = torch.load(path)\n",
    "        model.cuda()\n",
    "        model.args.use_gpu = True\n",
    "    else:\n",
    "        model = torch.load(path, map_location=torch.device('cpu'))\n",
    "        model.args.use_gpu = False\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model_dir = './Data/gqa/modelts4_es4_template102_md3_recnone_iterPerRound5_numRound3000_mtmax_filterConstants80_feat30_gpuTrue_lrbg0.001_lri0.01_lrr0.01_tgls0_filterIndirectFalse_embWN_randomIPP5_splitD2'\n",
    "model = load_model(model_dir, 'person', False)\n",
    "model_embs = model.embeddings_bgs[wn_pred_id, :]\n",
    "plot_dist_mat(model_embs[:tot,:].detach(), bg_pred_ls[:tot])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "cnt=20\n",
    "tot=10\n",
    "tot = min(wn_embs.shape[0], tot)\n",
    "show_dist=True\n",
    "get_closest_pairs(wn_embs[:tot,:].detach(), bg_pred_ls[:tot], cnt, show_dist=show_dist)\n",
    "get_closest_pairs(model_embs[:tot,:].detach(), bg_pred_ls[:tot], cnt, show_dist=show_dist)\n",
    "show_dist=False\n",
    "get_closest_pairs(wn_embs[:tot,:].detach(), bg_pred_ls[:tot], cnt, show_dist=show_dist)\n",
    "get_closest_pairs(model_embs[:tot,:].detach(), bg_pred_ls[:tot], cnt, show_dist=show_dist)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0.5161124467849731, ('bag', 'backpack')), (0.438531756401062, ('arrow', 'apple')), (0.40217262506484985, ('airplane', 'air')), (0.3558805286884308, ('backpack', 'airplane')), (0.3506677746772766, ('apple', 'airplane')), (0.34332045912742615, ('animal', 'airplane')), (0.34238290786743164, ('arrow', 'animal')), (0.3387574851512909, ('at', 'above')), (0.3335779309272766, ('apple', 'animal')), (0.3329431712627411, ('bag', 'airplane')), (0.3198569715023041, ('arrow', 'airplane')), (0.2955162525177002, ('arm', 'airplane')), (0.27776288986206055, ('arm', 'apple')), (0.27376842498779297, ('animal', 'air')), (0.27369213104248047, ('arrow', 'arm')), (0.26726627349853516, ('backpack', 'arm')), (0.2658688724040985, ('bag', 'air')), (0.2648478150367737, ('arrow', 'above')), (0.2648349106311798, ('bag', 'arrow')), (0.261461079120636, ('bag', 'animal'))]\n",
      "[(0.8734731674194336, ('bag', 'backpack')), (0.8344952464103699, ('bag', 'airplane')), (0.8223341107368469, ('arrow', 'apple')), (0.8121474385261536, ('airplane', 'air')), (0.7854490280151367, ('arm', 'air')), (0.7778609395027161, ('backpack', 'airplane')), (0.7336782813072205, ('air', 'above')), (0.7281067371368408, ('apple', 'air')), (0.7059981822967529, ('arrow', 'animal')), (0.7053775787353516, ('arm', 'airplane')), (0.698470950126648, ('apple', 'animal')), (0.6939692497253418, ('arrow', 'above')), (0.68451327085495, ('apple', 'airplane')), (0.674484133720398, ('bag', 'above')), (0.6726108193397522, ('animal', 'air')), (0.6704418063163757, ('bag', 'air')), (0.6691268086433411, ('arrow', 'air')), (0.6668235063552856, ('bag', 'apple')), (0.6628450751304626, ('bag', 'arrow')), (0.6592417359352112, ('bag', 'arm'))]\n",
      "[('bag', 'backpack'), ('arrow', 'apple'), ('airplane', 'air'), ('backpack', 'airplane'), ('apple', 'airplane'), ('animal', 'airplane'), ('arrow', 'animal'), ('at', 'above'), ('apple', 'animal'), ('bag', 'airplane'), ('arrow', 'airplane'), ('arm', 'airplane'), ('arm', 'apple'), ('animal', 'air'), ('arrow', 'arm'), ('backpack', 'arm'), ('bag', 'air'), ('arrow', 'above'), ('bag', 'arrow'), ('bag', 'animal')]\n",
      "[('bag', 'backpack'), ('bag', 'airplane'), ('arrow', 'apple'), ('airplane', 'air'), ('arm', 'air'), ('backpack', 'airplane'), ('air', 'above'), ('apple', 'air'), ('arrow', 'animal'), ('arm', 'airplane'), ('apple', 'animal'), ('arrow', 'above'), ('apple', 'airplane'), ('bag', 'above'), ('animal', 'air'), ('bag', 'air'), ('arrow', 'air'), ('bag', 'apple'), ('bag', 'arrow'), ('bag', 'arm')]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8b60173cfa7c2e39e9fe0a598a4a9a8996706931c07bbe2ea8d9cf3896c98fad"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('torch1.8cuda': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}